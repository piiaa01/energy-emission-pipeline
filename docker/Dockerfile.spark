FROM bitnamilegacy/spark:3.5.0

USER root

WORKDIR /app

# Python deps (PySpark ist im Image schon drin, aber pymongo brauchen wir)
COPY docker/requirements-spark.txt ./requirements-spark.txt
RUN pip install --no-cache-dir -r requirements-spark.txt

# Streaming job
COPY spark/streaming_job.py ./streaming_job.py

ENV PYTHONUNBUFFERED=1

# Run Spark with Kafka integration (download connector via --packages)
CMD ["/opt/bitnami/spark/bin/spark-submit", \
     "--master", "local[*]", \
     "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0", \
     "/app/streaming_job.py"]
